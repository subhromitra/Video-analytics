{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"C:\\\\Users\\\\smitr\\\\Desktop\\\\ucf10_resnetFeat\\\\train\"\n",
    "test_path  = \"C:\\\\Users\\\\smitr\\\\Desktop\\\\ucf10_resnetFeat\\\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(train_path)\n",
    "classes.sort()\n",
    "labels = np.arange(10)  # For 10 classes\n",
    "trainShuffList = []\n",
    "labelShuffList = []\n",
    "\n",
    "for c in range(10):\n",
    "    files = os.listdir(train_path+\"\\\\\"+classes[c])\n",
    "    for f in files:\n",
    "        trainShuffList.append(classes[c]+\"\\\\\"+f)\n",
    "        labelShuffList.append(float(labels[c]))\n",
    "# Shuffling data list and label list\n",
    "trainList = list(zip(trainShuffList, labelShuffList))\n",
    "shuffle(trainList)\n",
    "trainShuffList, labelShuffList = zip(*trainList)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BaseballPitch\\\\v_BaseballPitch_g14_c02.pt',\n",
       " 'ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g02_c02.pt',\n",
       " 'BandMarching\\\\v_BandMarching_g12_c05.pt',\n",
       " 'BasketballDunk\\\\v_BasketballDunk_g01_c06.pt',\n",
       " 'ApplyLipstick\\\\v_ApplyLipstick_g17_c02.pt',\n",
       " 'BaseballPitch\\\\v_BaseballPitch_g06_c03.pt',\n",
       " 'Basketball\\\\v_Basketball_g03_c03.pt',\n",
       " 'Archery\\\\v_Archery_g05_c04.pt',\n",
       " 'Archery\\\\v_Archery_g06_c02.pt',\n",
       " 'BaseballPitch\\\\v_BaseballPitch_g19_c01.pt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainShuffList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test list for loading feature tensors\n",
    "testList = []\n",
    "testLabelList = []\n",
    "\n",
    "for c in range(10):\n",
    "    files = os.listdir(test_path+\"\\\\\"+classes[c])\n",
    "    for f in files:\n",
    "        testList.append(classes[c]+\"\\\\\"+f)\n",
    "        testLabelList.append(float(labels[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define LSTM architecture </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net_LSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, nLayers, nClasses):\n",
    "        super(net_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_sz, hidden_sz, nLayers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_sz, nClasses)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        # Output of hidden state from last time step\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define train routine </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, inputs, labels, optimizer, criterion):\n",
    "    net.train(True)\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Initialize gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Compute error\n",
    "    loss = criterion(F.log_softmax(outputs), labels)\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    # Update params\n",
    "    optimizer.step()\n",
    "    if use_gpu:\n",
    "        correct = (predicted.cpu() == labels.data.cpu()).sum().item()\n",
    "    else:\n",
    "        correct = (predicted == labels.data).sum().item()\n",
    "    return net, loss.data.item(), correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define test routine </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, inputs, labels, criterion):\n",
    "    net.train(False)\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Compute loss \n",
    "    loss = criterion(F.log_softmax(outputs), labels)\n",
    "    if use_gpu:\n",
    "        correct = (predicted.cpu() == labels.data.cpu()).sum().item()\n",
    "    else:\n",
    "        correct = (predicted == labels.data).sum().item()\n",
    "    return loss.data.item(), correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Initialize the network </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net_LSTM(512, 256, 2, 10)\n",
    "if use_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define loss function & optimizer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training the network </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\smitr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\users\\smitr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 /3000;  Training Loss: 0.073188 ; Training Acc: 12.397\n",
      "Iteration: 200 /3000;  Testing Loss: 0.074294 ; Testing Acc: 3.971\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 400 /3000;  Training Loss: 0.073247 ; Training Acc: 11.304\n",
      "Iteration: 400 /3000;  Testing Loss: 0.074433 ; Testing Acc: 11.552\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 600 /3000;  Training Loss: 0.073095 ; Training Acc: 11.668\n",
      "Iteration: 600 /3000;  Testing Loss: 0.074260 ; Testing Acc: 4.693\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 800 /3000;  Training Loss: 0.073245 ; Training Acc: 10.939\n",
      "Iteration: 800 /3000;  Testing Loss: 0.074374 ; Testing Acc: 10.830\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 1000 /3000;  Training Loss: 0.071666 ; Training Acc: 13.856\n",
      "Iteration: 1000 /3000;  Testing Loss: 0.074342 ; Testing Acc: 10.830\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 1200 /3000;  Training Loss: 0.038205 ; Training Acc: 49.954\n",
      "Iteration: 1200 /3000;  Testing Loss: 0.046877 ; Testing Acc: 39.350\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 1400 /3000;  Training Loss: 0.041257 ; Training Acc: 45.397\n",
      "Iteration: 1400 /3000;  Testing Loss: 0.047228 ; Testing Acc: 42.599\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 1600 /3000;  Training Loss: 0.028977 ; Training Acc: 63.902\n",
      "Iteration: 1600 /3000;  Testing Loss: 0.044363 ; Testing Acc: 44.043\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 1800 /3000;  Training Loss: 0.023583 ; Training Acc: 68.915\n",
      "Iteration: 1800 /3000;  Testing Loss: 0.052047 ; Testing Acc: 46.209\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 2000 /3000;  Training Loss: 0.018817 ; Training Acc: 76.937\n",
      "Iteration: 2000 /3000;  Testing Loss: 0.063181 ; Testing Acc: 44.404\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 2200 /3000;  Training Loss: 0.013927 ; Training Acc: 83.409\n",
      "Iteration: 2200 /3000;  Testing Loss: 0.052818 ; Testing Acc: 48.736\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 2400 /3000;  Training Loss: 0.006611 ; Training Acc: 92.890\n",
      "Iteration: 2400 /3000;  Testing Loss: 0.058728 ; Testing Acc: 50.181\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 2600 /3000;  Training Loss: 0.007147 ; Training Acc: 92.343\n",
      "Iteration: 2600 /3000;  Testing Loss: 0.064285 ; Testing Acc: 53.791\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 2800 /3000;  Training Loss: 0.003401 ; Training Acc: 96.901\n",
      "Iteration: 2800 /3000;  Testing Loss: 0.062220 ; Testing Acc: 56.679\n",
      "Time consumed: 0m 2s\n",
      "Iteration: 3000 /3000;  Training Loss: 0.001992 ; Training Acc: 98.997\n",
      "Iteration: 3000 /3000;  Testing Loss: 0.051975 ; Testing Acc: 56.318\n",
      "Time consumed: 0m 2s\n",
      "Training completed in 138m 34s\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "bSize = 32 # Batch size\n",
    "L = 32 # Number of time steps\n",
    "\n",
    "bCount = len(trainShuffList)//bSize # Number of batches in train set\n",
    "lastBatch = len(trainShuffList)%bSize # Number of samples in last batch of train set\n",
    "\n",
    "test_bCount = len(testList)//bSize # Number of batches in test set\n",
    "test_lastBatch = len(testList)%bSize # Number of samples in last batch of test set\n",
    "\n",
    "# Lists for saving train/test loss and accuracy\n",
    "trainLoss = []\n",
    "trainAcc = []\n",
    "testLoss = []\n",
    "testAcc = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epochNum in range(epochs):\n",
    "    # Shuffling train data for each epoch\n",
    "    trainList = list(zip(trainShuffList, labelShuffList))\n",
    "    shuffle(trainList)\n",
    "    trainShuffList, labelShuffList = zip(*trainList)\n",
    "    \n",
    "    trainRunLoss = 0.0\n",
    "    testRunLoss = 0.0\n",
    "    trainRunCorr = 0\n",
    "    testRunCorr = 0\n",
    "    \n",
    "    epochStart = time.time()\n",
    "    \n",
    "    ## Train\n",
    "    # Load data tensors batchwise     \n",
    "    idx = 0    \n",
    "    for bNum in range(bCount):\n",
    "        first = True\n",
    "        # Loading one batch\n",
    "        for dNum in range(idx,idx+bSize):\n",
    "            if first:\n",
    "                loadData = torch.load(train_path+\"\\\\\"+trainShuffList[dNum])\n",
    "                sz = loadData.size(0)   # No.of images extracted from video\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                batchData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchLabel = torch.Tensor([labelShuffList[dNum]]).long()                          \n",
    "                first = False                \n",
    "            else:\n",
    "                loadData = torch.load(train_path+\"\\\\\"+trainShuffList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                tempData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchData = torch.cat((batchData,tempData), dim=0)\n",
    "                batchLabel = torch.cat((batchLabel,torch.Tensor([labelShuffList[dNum]]).long()),dim=0)            \n",
    "        \n",
    "        # Train the network on current batch\n",
    "        net, tr_loss, tr_corr = train(net, batchData, batchLabel, optimizer, criterion)\n",
    "        trainRunLoss += tr_loss\n",
    "        trainRunCorr += tr_corr\n",
    "        idx += bSize\n",
    "        \n",
    "    # Loading last batch\n",
    "    if lastBatch != 0:        \n",
    "        first = True\n",
    "        for dNum in range(idx,idx+lastBatch):\n",
    "            if first:\n",
    "                loadData = torch.load(train_path+\"\\\\\"+trainShuffList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                batchData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchLabel = torch.Tensor([labelShuffList[dNum]]).long()\n",
    "                first = False                \n",
    "            else:\n",
    "                loadData = torch.load(train_path+\"\\\\\"+trainShuffList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                tempData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchData = torch.cat((batchData,tempData), dim=0)\n",
    "                batchLabel = torch.cat((batchLabel,torch.Tensor([labelShuffList[dNum]]).long()),dim=0)          \n",
    "        \n",
    "        # Training network on last batch\n",
    "        net, tr_loss, tr_corr = train(net, batchData, batchLabel, optimizer, criterion)\n",
    "        trainRunLoss += tr_loss\n",
    "        trainRunCorr += tr_corr\n",
    "    \n",
    "    # Average training loss and accuracy for each epoch\n",
    "    avgTrainLoss = trainRunLoss/float(len(trainShuffList))\n",
    "    trainLoss.append(avgTrainLoss)\n",
    "    avgTrainAcc = trainRunCorr/float(len(trainShuffList))\n",
    "    trainAcc.append(avgTrainAcc)\n",
    "    \n",
    "    ## Test\n",
    "    # Load data tensors batchwise     \n",
    "    idx = 0    \n",
    "    for bNum in range(test_bCount):\n",
    "        first = True\n",
    "        # Loading one batch\n",
    "        for dNum in range(idx,idx+bSize): \n",
    "            if first:\n",
    "                loadData = torch.load(test_path+\"\\\\\"+testList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                batchData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchLabel = torch.Tensor([testLabelList[dNum]]).long()\n",
    "                first = False                \n",
    "            else:\n",
    "                loadData = torch.load(test_path+\"\\\\\"+testList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                tempData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchData = torch.cat((batchData,tempData), dim=0)\n",
    "                batchLabel = torch.cat((batchLabel,torch.Tensor([testLabelList[dNum]]).long()),dim=0)            \n",
    "        \n",
    "        # Test the network on current batch\n",
    "        ts_loss, ts_corr = test(net, batchData, batchLabel, criterion)\n",
    "        testRunLoss += ts_loss\n",
    "        testRunCorr += ts_corr\n",
    "        idx += bSize\n",
    "     \n",
    "    # Loading last batch    \n",
    "    if test_lastBatch != 0:        \n",
    "        first = True\n",
    "        for dNum in range(idx,idx+test_lastBatch):\n",
    "            if first:\n",
    "                loadData = torch.load(test_path+\"\\\\\"+testList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                batchData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)               \n",
    "                batchLabel = torch.Tensor([testLabelList[dNum]]).long()\n",
    "                first = False                \n",
    "            else:\n",
    "                loadData = torch.load(test_path+\"\\\\\"+testList[dNum])\n",
    "                sz = loadData.size(0)\n",
    "                idx1 = torch.from_numpy(np.arange(0,(sz//L)*L,sz//L))\n",
    "                tempData = torch.index_select(loadData,dim=0,index=idx1.long()).unsqueeze(0)\n",
    "                batchData = torch.cat((batchData,tempData), dim=0)\n",
    "                batchLabel = torch.cat((batchLabel,torch.Tensor([testLabelList[dNum]]).long()),dim=0)          \n",
    "        \n",
    "        # Test network on last batch\n",
    "        ts_loss, ts_corr = test(net, batchData, batchLabel, criterion)\n",
    "        testRunLoss += ts_loss\n",
    "        testRunCorr += tr_corr\n",
    "        \n",
    "    # Average testing loss and accuracy for each epoch\n",
    "    avgTestLoss = testRunLoss/float(len(testList))\n",
    "    testLoss.append(avgTestLoss)\n",
    "    avgTestAcc = testRunCorr/float(len(testList))\n",
    "    testAcc.append(avgTestAcc)   \n",
    "    \n",
    "    \n",
    "    if (epochNum+1) % 200 == 0:\n",
    "        epochEnd = time.time()-epochStart\n",
    "        print('Iteration: {:.0f} /{:.0f};  Training Loss: {:.6f} ; Training Acc: {:.3f}'\\\n",
    "              .format(epochNum + 1,epochs, avgTrainLoss, avgTrainAcc*100))\n",
    "        print('Iteration: {:.0f} /{:.0f};  Testing Loss: {:.6f} ; Testing Acc: {:.3f}'\\\n",
    "              .format(epochNum + 1,epochs, avgTestLoss, avgTestAcc*100))\n",
    "    \n",
    "        print('Time consumed: {:.0f}m {:.0f}s'.format(epochEnd//60,epochEnd%60))        \n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss vs Epochs\n",
    "fig1 = plt.figure(1)        \n",
    "plt.plot(range(epochs),trainLoss,'r-',label='train')  \n",
    "plt.plot(range(epochs),testLoss,'g-',label='test') \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')   \n",
    "\n",
    "# Plotting testing accuracy vs Epochs\n",
    "fig2 = plt.figure(2)        \n",
    "plt.plot(range(epochs),trainAcc,'r-',label='train')    \n",
    "plt.plot(range(epochs),testAcc,'g-',label='test')        \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
